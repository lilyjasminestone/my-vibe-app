import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from openai import AsyncOpenAI

from backend.config.settings import settings
from backend.utils.logger import logger


def _debug_print_messages(messages: List[Dict], title: str = "LLM Context"):
    """è°ƒè¯•æ¨¡å¼ä¸‹ç¾åŒ–è¾“å‡º LLM æ¶ˆæ¯"""
    if not settings.debug:
        return

    # é¢œè‰²å®šä¹‰
    colors = {
        "system": {"icon": "âš™ï¸", "color": "\033[1;33m", "bg": "\033[43m\033[30m"},
        "user": {"icon": "ğŸ‘¤", "color": "\033[1;32m", "bg": "\033[42m\033[37m"},
        "assistant": {"icon": "ğŸ¤–", "color": "\033[1;34m", "bg": "\033[44m\033[37m"},
        "function": {"icon": "âš¡", "color": "\033[1;35m", "bg": "\033[45m\033[37m"},
        "tool": {"icon": "ğŸ”§", "color": "\033[1;36m", "bg": "\033[46m\033[30m"},
        "developer": {"icon": "ğŸ‘©â€ğŸ’»", "color": "\033[1;31m", "bg": "\033[41m\033[37m"},
    }

    reset = "\033[0m"

    # æ ‡é¢˜
    title_color = "\033[1;36m"
    print(f"\n{title_color}ğŸš€ {title} ({len(messages)} messages){reset}")
    print(f"{title_color}{'=' * 50}{reset}")

    # æ¶ˆæ¯å†…å®¹
    for i, item in enumerate(messages, 1):
        role = item.get("role", "unknown").lower()
        content = item.get("content", "").strip()

        # è·å–è§’è‰²é…ç½®
        role_config = colors.get(
            role, {"icon": "â“", "color": "\033[1;37m", "bg": "\033[47m\033[30m"}
        )

        # è§’è‰²æ ‡ç­¾
        role_display = role.upper().ljust(9)
        role_tag = f"{role_config['bg']} {role_display} {reset}"

        # å­—ç¬¦ç»Ÿè®¡é¢œè‰²
        char_count = len(content)
        if char_count > 200:
            count_color = "\033[1;31m"  # çº¢è‰²
        elif char_count > 100:
            count_color = "\033[1;33m"  # é»„è‰²
        else:
            count_color = "\033[1;32m"  # ç»¿è‰²

        # æ¶ˆæ¯å¤´
        print(
            f"\n{i:2d}. {role_config['icon']} {role_tag} {count_color}[{char_count} chars]{reset}"
        )

        # å†…å®¹å¤„ç†ï¼šå®Œæ•´æ˜¾ç¤ºæ‰€æœ‰è¡Œ
        if content:
            lines = content.split("\n")
            for line in lines:
                print(f"    {role_config['color']}{line}{reset}")
        else:
            print(f"    {role_config['color']}(empty){reset}")

    print(f"\n{title_color}{'=' * 50}{reset}")


class LLMClient:
    """LLM å…¬å…±å®¢æˆ·ç«¯ç»„ä»¶ - ä½¿ç”¨OpenAIåŒ…"""

    def __init__(self, base_url: Optional[str] = None, api_key: Optional[str] = None):
        self.base_url = base_url or settings.llm_base_url
        self.api_key = api_key or settings.llm_api_key

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

    async def aclose(self):
        """å…³é—­å®¢æˆ·ç«¯è¿æ¥"""
        try:
            await self.client.close()
        except Exception as e:
            logger.warning(f"å…³é—­ LLM å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")

    async def chat_completion(
        self,
        message: str,
        model: str,
        temperature: float,
        session_id: str,
        trace_id: str,
        user_id: str,
        context: Optional[List[Dict[str, str]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:
        if not self.api_key:
            return {"success": False, "error": "API Key æœªé…ç½®"}

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        messages = context.copy() if context else []
        messages.append({"role": "user", "content": message})

        _debug_print_messages(messages, "LLM Chat Completion")

        try:
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
            completion_args = {
                "model": model,
                "messages": messages,
                "max_tokens": 4096,
                "temperature": temperature,
            }

            # æ·»åŠ  Function Calling æ”¯æŒ
            if tools:
                completion_args["tools"] = tools
                completion_args["tool_choice"] = "auto"

            response = await self.client.chat.completions.create(**completion_args)

            response_content = response.choices[0].message.content
            prompt_tokens = response.usage.prompt_tokens if response.usage else None
            completion_tokens = (
                response.usage.completion_tokens if response.usage else None
            )
            total_tokens = response.usage.total_tokens if response.usage else None

            logger.info(
                f"LLM tokens - prompt: {prompt_tokens}, completion: {completion_tokens}, total: {total_tokens}"
            )
            logger.info(
                f"session_id: {session_id}, trace_id: {trace_id}, user_id: {user_id}"
            )

            result = {
                "success": True,
                "response": response_content,
                "model": model,
            }

            # å¦‚æœæœ‰ tool_callsï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if response.choices[0].message.tool_calls:
                result["tool_calls"] = []
                for tool_call in response.choices[0].message.tool_calls:
                    result["tool_calls"].append(
                        {
                            "id": tool_call.id,
                            "type": tool_call.type,
                            "function": {
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments,
                            },
                        }
                    )

            return result

        except Exception as e:
            logger.error(f"LLM è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return {"success": False, "error": f"è¯·æ±‚å¼‚å¸¸: {str(e)}"}

    async def chat_completion_sse(
        self,
        message: str,
        model: str,
        temperature: float,
        session_id: str,
        trace_id: str,
        user_id: str,
        context: Optional[List[dict]] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """
        SSEæµå¼èŠå¤©è¯·æ±‚ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ‹¼æ¥ - ä½¿ç”¨OpenAIåŒ…
        """
        if not self.api_key:
            yield {"error": "API Key æœªé…ç½®"}
            return

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆæ‹¼æ¥ä¸Šä¸‹æ–‡ï¼‰
        messages = context.copy() if context else []
        messages.append({"role": "user", "content": message})

        _debug_print_messages(messages, "LLM Stream Chat")

        try:
            full_response = ""
            prompt_tokens = None
            completion_tokens = None
            total_tokens = None
            completion_start_time = None

            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯åˆ›å»ºæµå¼å“åº”
            # æ·»åŠ  timeout é˜²æ­¢è¯·æ±‚å¡æ­»
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=4096,
                temperature=temperature,
                stream=True,
                timeout=60.0 # 60s timeout
            )

            async for chunk in stream:
                if completion_start_time is None:
                    completion_start_time = datetime.now()

                # æ£€æŸ¥æ˜¯å¦åŒ…å« usage ä¿¡æ¯
                if hasattr(chunk, "usage") and chunk.usage:
                    prompt_tokens = chunk.usage.prompt_tokens
                    completion_tokens = chunk.usage.completion_tokens
                    total_tokens = chunk.usage.total_tokens

                # æå–å¢é‡å†…å®¹
                if chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    if choice.delta and choice.delta.content:
                        delta = choice.delta.content
                        full_response += delta
                        yield {"success": True, "delta": delta}

                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if choice.finish_reason in ["stop", "length", "content_filter"]:
                        break

            # è¾“å‡º token ç»Ÿè®¡ä¿¡æ¯
            if prompt_tokens is not None or completion_tokens is not None:
                logger.info(
                    f"LLM tokens - prompt: {prompt_tokens}, completion: {completion_tokens}, total: {total_tokens}"
                )
                logger.info(
                    f"session_id: {session_id}, trace_id: {trace_id}, user_id: {user_id}"
                )

        except Exception as e:
            logger.error(f"LLM æµå¼è¯·æ±‚å¼‚å¸¸: {str(e)}")
            yield {"success": False, "error": f"è¯·æ±‚å¼‚å¸¸: {str(e)}"}

    def get_config_info(self) -> Dict[str, Any]:
        return {
            "base_url": self.base_url,
            "api_key_configured": bool(self.api_key),
            "api_key_length": len(self.api_key) if self.api_key else 0,
        }
